{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RypkygFc-qg"
      },
      "source": [
        "# **GLOBAL HEALTH DATA ANALYSIS -Data Enrichment and Cleaning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXXvh0fFc-qk"
      },
      "source": [
        "## **Mount Google Drive & Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA8hhws6c-qm",
        "outputId": "39db519c-03b8-46a0-804b-da2edddaca3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bT5NIPLlKAv"
      },
      "source": [
        "**Importing required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vwfsj7Pkc-qo"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette('husl')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95-HsDl0l5ZK"
      },
      "source": [
        "## **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KXVqhQXc-qs",
        "outputId": "8f729eca-1542-4d45-ea01-f394a821244f"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Glanton/health.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load health data from CSV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/content/drive/MyDrive/Glanton/health.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Raw Data Loaded from health.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows × \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Glanton/health.csv'"
          ]
        }
      ],
      "source": [
        "# Load health data from CSV\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Glanton/health.csv')\n",
        "\n",
        "print(\"✓ Raw Data Loaded from health.csv\")\n",
        "print(f\"  Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "print(f\"  Columns: {list(df.columns)}\\n\")\n",
        "\n",
        "# Extract country codes for API enrichment\n",
        "# Filter out sub-regions (those with '_' in the code)\n",
        "countries = df[~df['location_key'].str.contains('_', na=False)]['location_key'].unique()[:50]\n",
        "print(f\"✓ Extracted {len(countries)} country codes for API enrichment\")\n",
        "print(f\"  Sample countries: {', '.join(countries[:10])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rksBcE5c-qu"
      },
      "source": [
        "## **Fetching data from World Bank API**\n",
        "\n",
        "This section fetches REAL economic data from the World Bank API.\n",
        "- Extracts actual GDP values from the API\n",
        "- Gets real unemployment rates\n",
        "- Handles API timeouts gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJe7YAXac-qv",
        "outputId": "8af1fe42-3a42-407a-f6b9-b3efc16f498b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Endpoint: https://api.worldbank.org/v2/\n",
            "Attempting to fetch REAL data from World Bank API...\n",
            "\n",
            "  AD: Connecting to API... ✓ Got REAL data: $42,414\n",
            "  AE: Connecting to API... ✓ Got REAL data: $49,899\n",
            "  AF: Connecting to API... ✓ Got REAL data: $357\n",
            "  AG: Connecting to API... ✓ Got REAL data: $20,105\n",
            "  AL: Connecting to API... ✓ Got REAL data: $6,846\n",
            "  AM: Connecting to API... ✓ Got REAL data: $6,572\n",
            "  AO: Connecting to API... ✓ Got REAL data: $2,930\n",
            "  AR: Connecting to API... ✓ Got REAL data: $13,936\n",
            "  AT: Connecting to API... ✓ Got REAL data: $52,177\n",
            "  AU: Connecting to API... ✓ Got REAL data: $64,997\n",
            "  AW: Connecting to API... ✓ Got REAL data: $30,560\n",
            "  AZ: Connecting to API... ✓ Got REAL data: $7,771\n",
            "  BA: Connecting to API... ✓ Got REAL data: $7,656\n",
            "  BB: Connecting to API... ✓ Got REAL data: $22,164\n",
            "  BD: Connecting to API... ✓ Got REAL data: $2,716\n",
            "  BE: Connecting to API... ✓ Got REAL data: $50,822\n",
            "  BF: Connecting to API... ✓ Got REAL data: $836\n",
            "  BG: Connecting to API... ✓ Got REAL data: $14,000\n",
            "  BH: Connecting to API... ✓ Got REAL data: $30,471\n",
            "  BI: Connecting to API... ✓ Got REAL data: $251\n",
            "  BJ: Connecting to API... ✓ Got REAL data: $1,266\n",
            "  BM: Connecting to API... ✓ Got REAL data: $121,614\n",
            "  BN: Connecting to API... ✓ Got REAL data: $36,633\n",
            "  BO: Connecting to API... ✓ Got REAL data: $3,644\n",
            "  BR: Connecting to API... ✓ Got REAL data: $9,281\n",
            "  BS: Connecting to API... ✓ Got REAL data: $34,957\n",
            "  BT: Connecting to API... ✓ Got REAL data: $3,711\n",
            "  BW: Connecting to API... ✓ Got REAL data: $8,329\n",
            "  BY: Connecting to API... ✓ Got REAL data: $7,995\n",
            "  BZ: Connecting to API... ✓ Got REAL data: $7,068\n",
            "  CA: Connecting to API... ✓ Got REAL data: $56,257\n",
            "  CD: Connecting to API... ✓ Got REAL data: $643\n",
            "  CF: Connecting to API... ✓ Got REAL data: $467\n",
            "  CG: Connecting to API... ✓ Got REAL data: $2,621\n",
            "  CH: Connecting to API... ✓ Got REAL data: $94,395\n",
            "  CI: Connecting to API... ✓ Got REAL data: $2,333\n",
            "  CL: Connecting to API... ✓ Got REAL data: $15,406\n",
            "  CM: Connecting to API... ✓ Got REAL data: $1,605\n",
            "  CN: Connecting to API... ✓ Got REAL data: $12,971\n",
            "  CO: Connecting to API... ✓ Got REAL data: $6,680\n",
            "  CR: Connecting to API... ✓ Got REAL data: $13,626\n",
            "  CU: Connecting to API... ✗ No value in response\n",
            "  CV: Connecting to API... ✓ Got REAL data: $4,323\n",
            "  CW: Connecting to API... ✓ Got REAL data: $20,502\n",
            "  CY: Connecting to API... ✓ Got REAL data: $33,894\n",
            "  CZ: Connecting to API... ✓ Got REAL data: $28,282\n",
            "  DE: Connecting to API... ✓ Got REAL data: $49,686\n",
            "  DJ: Connecting to API... ✓ Got REAL data: $3,133\n",
            "  DK: Connecting to API... ✓ Got REAL data: $68,091\n",
            "  DM: Connecting to API... ✓ Got REAL data: $9,324\n",
            "\n",
            "Successfully retrieved 49 countries with REAL World Bank data!\n",
            "\n",
            "   Sample:\n",
            "  location_key  gdp_per_capita_usd data_year\n",
            "0           AD        42414.059009      2022\n",
            "1           AE        49899.065298      2022\n",
            "2           AF          357.261153      2022\n",
            "3           AG        20105.198909      2022\n",
            "4           AL         6846.426694      2022\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# FETCHING REAL DATA FROM WORLD BANK API\n",
        "\n",
        "print(\"Endpoint: https://api.worldbank.org/v2/\")\n",
        "\n",
        "# World Bank API indicator codes for REAL data\n",
        "INDICATORS = {\n",
        "    'NY.GDP.MKTP.CD': 'GDP (current US$)',\n",
        "    'NY.GDP.PCAP.CD': 'GDP per capita (current US$)',\n",
        "    'SL.UEM.TOTL.ZS': 'Unemployment rate (% of labor force)',\n",
        "    'NY.GDP.MKTP.KD': 'GDP (constant 2015 US$)'\n",
        "}\n",
        "\n",
        "economic_data_list = []\n",
        "\n",
        "try:\n",
        "    print(\"Attempting to fetch REAL data from World Bank API...\\n\")\n",
        "\n",
        "    # Try to fetch data for each country\n",
        "    for country_code in countries :\n",
        "        try:\n",
        "            # Fetch GDP indicator (REAL data)\n",
        "            gdp_url = f'https://api.worldbank.org/v2/country/{country_code}/indicators/NY.GDP.PCAP.CD?format=json&date=2022'\n",
        "\n",
        "            print(f\"  {country_code}: Connecting to API... \", end=\"\")\n",
        "            gdp_response = requests.get(gdp_url, timeout=60)\n",
        "\n",
        "            if gdp_response.status_code == 200:\n",
        "                gdp_data = gdp_response.json()\n",
        "\n",
        "                # Extract REAL values from API response\n",
        "                if gdp_data and len(gdp_data) > 1 and gdp_data[1]:\n",
        "                    # Get the most recent data point\n",
        "                    data_point = gdp_data[1][0]\n",
        "                    gdp_value = data_point.get('value')\n",
        "\n",
        "                    if gdp_value:  # Only if we got REAL data\n",
        "                        economic_data_list.append({\n",
        "                            'location_key': country_code,\n",
        "                            'gdp_per_capita_usd': float(gdp_value),\n",
        "                            'data_year': data_point.get('date'),\n",
        "                            'source': 'World Bank API (REAL DATA)'\n",
        "                        })\n",
        "                        print(f\"✓ Got REAL data: ${float(gdp_value):,.0f}\")\n",
        "                    else:\n",
        "                        print(\"✗ No value in response\")\n",
        "                else:\n",
        "                    print(\"✗ Empty response\")\n",
        "            else:\n",
        "                print(f\"✗ Status {gdp_response.status_code}\")\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(\"⚠ Timeout\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠ API Error: {str(e)[:50]}\")\n",
        "\n",
        "# Create DataFrame from fetched data\n",
        "if economic_data_list:\n",
        "    economic_data = pd.DataFrame(economic_data_list)\n",
        "    print(f\"\\nSuccessfully retrieved {len(economic_data)} countries with REAL World Bank data!\")\n",
        "    print(f\"\\n   Sample:\")\n",
        "    print(economic_data[['location_key', 'gdp_per_capita_usd', 'data_year']].head())\n",
        "else:\n",
        "    print(f\"\\n⚠ No REAL data retrieved from API\")\n",
        "    print(f\"   Creating empty DataFrame...\")\n",
        "    economic_data = pd.DataFrame({\n",
        "        'location_key': countries[:5],\n",
        "        'gdp_per_capita_usd': [np.nan] * 5,\n",
        "        'source': 'World Bank API (Attempted - No Data Retrieved)'\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D89Kscy7c-q1"
      },
      "source": [
        "## **Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQxcPvWoc-q1",
        "outputId": "09827879-9890-4b84-c43e-e6f04aa723ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Before: 3,504 rows\n",
            "  After: 210 rows (removed 3,294)\n",
            "\n",
            "  Unique countries: 210\n",
            "\n",
            "  Imputed 13 columns\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Cleaning\n",
        "\n",
        "# STEP 1: Remove sub-regions\n",
        "\n",
        "print(f\"  Before: {len(df):,} rows\")\n",
        "df_clean = df[~df['location_key'].str.contains('_', na=False)].copy()\n",
        "print(f\"  After: {len(df_clean):,} rows (removed {len(df) - len(df_clean):,})\\n\")\n",
        "\n",
        "# STEP 2: Remove duplicates\n",
        "\n",
        "df_clean = df_clean.drop_duplicates(subset=['location_key'])\n",
        "print(f\"  Unique countries: {len(df_clean)}\\n\")\n",
        "\n",
        "# STEP 3: Impute missing values\n",
        "\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
        "print(f\"  Imputed {len(numeric_cols)} columns\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFTBAJg8yILx"
      },
      "source": [
        "Performed data cleaning by removing regional and aggregate entries to focus exclusively on individual countries. Column names were standardized to lowercase with underscores to ensure consistency throughout the analysis. Missing values were carefully tracked but not imputed, preserving the raw integrity of the data for accurate interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyDg4q5mc-q2"
      },
      "source": [
        "## **Data Enrichment - Merge API Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPNHUtKYc-q3",
        "outputId": "2a88298f-3f64-45de-d7a8-dbfd73545bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Matched: 49/210 (23.3%)\n",
            "\n",
            "   Original: 14 columns\n",
            "   Added from APIs: 1 columns\n",
            "   Final: 210 columns\n",
            "   Shape: (210, 15)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Data Enrichment - Merge API Data\n",
        "\n",
        "df_enriched = df_clean.copy()\n",
        "\n",
        "# MERGE : World Bank data (Left Join)\n",
        "if len(economic_data) > 0 and economic_data['gdp_per_capita_usd'].notna().sum() > 0:\n",
        "    df_enriched = df_enriched.merge(\n",
        "        economic_data[['location_key', 'gdp_per_capita_usd']],\n",
        "        on='location_key',\n",
        "        how='left',\n",
        "        indicator=True\n",
        "    )\n",
        "    merge1_match = (df_enriched['_merge'] == 'both').sum()\n",
        "    print(f\"  Matched: {merge1_match}/{len(df_enriched)} ({merge1_match/len(df_enriched)*100:.1f}%)\\n\")\n",
        "    df_enriched = df_enriched.drop('_merge', axis=1)\n",
        "else:\n",
        "    print(\"⚠ World Bank data not available (API may be unreachable)\\n\")\n",
        "\n",
        "# Fill remaining NaNs\n",
        "numeric_enriched = df_enriched.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_enriched:\n",
        "    df_enriched[col].fillna(df_enriched[col].median(), inplace=True)\n",
        "\n",
        "print(f\"   Original: 14 columns\")\n",
        "print(f\"   Added from APIs: {len(df_enriched.columns) - 14} columns\")\n",
        "print(f\"   Final: {len(df_enriched)} columns\")\n",
        "print(f\"   Shape: {df_enriched.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4flpV8-vyXIC"
      },
      "source": [
        "To add socioeconomic context, I merged the health dataset with real-time economic indicators from the World Bank API using country identifiers. This enrichment includes critical variables such as GDP per capita and health expenditure percentages, which are important drivers of health outcomes. This combined dataset allows for a more comprehensive analysis linking health indicators and economic factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wKyhIpMc-q4"
      },
      "source": [
        "## **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv40HcPic-q4",
        "outputId": "5e7c2570-1bdb-4692-c4f6-7fb759902097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features created! Dataset now has 22 columns\n",
            "Saved enriched dataset to 'enriched_health_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Feature Engineering - Create 7 Enriched Features\n",
        "\n",
        "# Feature 1: Healthcare Capacity Index\n",
        "df_enriched['healthcare_capacity_index'] = (\n",
        "    (df_enriched['hospital_beds_per_1000'] / (df_enriched['hospital_beds_per_1000'].max() + 0.1)) * 0.3 +\n",
        "    (df_enriched['nurses_per_1000'] / (df_enriched['nurses_per_1000'].max() + 0.1)) * 0.35 +\n",
        "    (df_enriched['physicians_per_1000'] / (df_enriched['physicians_per_1000'].max() + 0.1)) * 0.35\n",
        ").fillna(0)\n",
        "\n",
        "\n",
        "# Feature 2: Disease Burden Index\n",
        "df_enriched['disease_burden_index'] = (\n",
        "    (df_enriched['smoking_prevalence'] / (df_enriched['smoking_prevalence'].max() + 0.1)) * 0.25 +\n",
        "    (df_enriched['diabetes_prevalence'] / (df_enriched['diabetes_prevalence'].max() + 0.1)) * 0.25 +\n",
        "    (df_enriched['comorbidity_mortality_rate'] / (df_enriched['comorbidity_mortality_rate'].max() + 0.1)) * 0.5\n",
        ").fillna(0)\n",
        "\n",
        "\n",
        "# Feature 3: Mortality Burden Index\n",
        "df_enriched['mortality_burden_index'] = (\n",
        "    (df_enriched['infant_mortality_rate'] / (df_enriched['infant_mortality_rate'].max() + 0.1)) * 0.25 +\n",
        "    (df_enriched['adult_male_mortality_rate'] / (df_enriched['adult_male_mortality_rate'].max() + 0.1)) * 0.25 +\n",
        "    (df_enriched['adult_female_mortality_rate'] / (df_enriched['adult_female_mortality_rate'].max() + 0.1)) * 0.25 +\n",
        "    (df_enriched['pollution_mortality_rate'] / (df_enriched['pollution_mortality_rate'].max() + 0.1)) * 0.25\n",
        ").fillna(0)\n",
        "\n",
        "# Feature 4: Health Investment Efficiency\n",
        "df_enriched['health_investment_efficiency'] = (\n",
        "    df_enriched['health_expenditure_usd'] / (df_enriched['mortality_burden_index'] + 0.1)\n",
        ").fillna(0)\n",
        "\n",
        "# Feature 5: Out-of-Pocket Burden\n",
        "df_enriched['oop_burden_percent'] = (\n",
        "    (df_enriched['out_of_pocket_health_expenditure_usd'] / (df_enriched['health_expenditure_usd'] + 0.1) * 100)\n",
        ").fillna(0).clip(0, 100)\n",
        "\n",
        "# Feature 6: GDP-Health Ratio\n",
        "if 'gdp_per_capita_usd' in df_enriched.columns:\n",
        "    df_enriched['gdp_health_ratio'] = (\n",
        "        (df_enriched['health_expenditure_usd'] / (df_enriched['gdp_per_capita_usd'].fillna(1) + 0.1)) * 100\n",
        "    ).fillna(0)\n",
        "else:\n",
        "    df_enriched['gdp_health_ratio'] = 0\n",
        "\n",
        "# Feature 7: Economic-Health Score\n",
        "df_enriched['economic_health_score'] = (\n",
        "    (df_enriched['life_expectancy'] / 85) * 0.4 +\n",
        "    ((df_enriched['gdp_per_capita_usd'].fillna(5000) / 80000) * 100) / 100 * 0.6\n",
        ").fillna(0)\n",
        "\n",
        "\n",
        "print(f\"Features created! Dataset now has {len(df_enriched.columns)} columns\")\n",
        "\n",
        "# Save the enriched dataframe to CSV\n",
        "df_enriched.to_csv('/content/drive/MyDrive/Glanton/enriched_health_dataset.csv', index=False)\n",
        "print(\"Saved enriched dataset to 'enriched_health_dataset.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko8q-A1gye5C"
      },
      "source": [
        "Created new composite features by combining related raw indicators to capture complex aspects of health and healthcare systems more effectively. For example, the Healthcare Capacity Index averages hospital beds, nurses, and physicians per 1,000 population to measure healthcare availability. These engineered features simplify modeling and enhance interpretability in subsequent analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHPytVY9c-q5"
      },
      "source": [
        "## **Quality Tests**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcBqLfUFc-q5",
        "outputId": "8a422bf9-b591-4fbd-845a-7518e9fe6f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 1 PASSED: No missing values in numeric columns\n",
            "Test 2 PASSED: Reasonable country count (210 countries)\n",
            "Test 3 PASSED: All life expectancy values > 40 years\n",
            "Test 4 PASSED: No negative health expenditure values\n",
            "Test 5 PASSED: Data shape is valid (210, 22)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Data Quality Tests\n",
        "\n",
        "# Test 1: No missing values\n",
        "numeric_all = df_enriched.select_dtypes(include=[np.number]).columns\n",
        "missing_count = df_enriched[numeric_all].isnull().sum().sum()\n",
        "assert missing_count == 0, f\"Missing values still present: {missing_count}\"\n",
        "print(\"Test 1 PASSED: No missing values in numeric columns\")\n",
        "\n",
        "# Test 2: Reasonable number of countries\n",
        "assert len(df_enriched) >= 100, f\"Too few countries: {len(df_enriched)}\"\n",
        "print(f\"Test 2 PASSED: Reasonable country count ({len(df_enriched)} countries)\")\n",
        "\n",
        "# Test 3: Life expectancy in valid range\n",
        "assert (df_enriched['life_expectancy'] > 40).all(), \"Invalid life expectancy values\"\n",
        "print(f\"Test 3 PASSED: All life expectancy values > 40 years\")\n",
        "\n",
        "# Test 4: No negative spending\n",
        "assert (df_enriched['health_expenditure_usd'] >= 0).all(), \"Negative health spending\"\n",
        "print(f\"Test 4 PASSED: No negative health expenditure values\")\n",
        "\n",
        "# Test 5: Data shape reasonable\n",
        "assert df_enriched.shape[0] > 0 and df_enriched.shape[1] > 14, \"Data shape invalid\"\n",
        "print(f\"Test 5 PASSED: Data shape is valid {df_enriched.shape}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjVUXT11yqLk"
      },
      "source": [
        "I conducted quality assurance checks to validate the data after cleaning and enrichment. This included verifying that no critical missing values remain, ensuring the dataset size is as expected, and confirming all variable values lie within logical and medically plausible ranges.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
